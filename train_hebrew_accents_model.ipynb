{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD6BOhCOv0OK"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_xn1OeDKsiM1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Input, Conv2D, ReLU\n",
        "from tensorflow.keras.layers import MaxPool2D, Dropout, Reshape, Flatten, BatchNormalization\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import unicodedata\n",
        "import json\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data augumentation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u2gpBYyu3OSr"
      },
      "outputs": [],
      "source": [
        "def add_noise(data, noise_factor):\n",
        "    noise = np.random.randn(len(data))\n",
        "    augmented_data = data + noise_factor * noise\n",
        "    augmented_data = augmented_data.astype(type(data[0]))\n",
        "    return augmented_data\n",
        "\n",
        "def change_pitch(data, sampling_rate, pitch_factor):\n",
        "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
        "\n",
        "def change_speed(data, speed_factor):\n",
        "    return librosa.effects.time_stretch(data, speed_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now we generate data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gimatria(letters):\n",
        "    alphabet = 'אבגדהוזחטיכלמנסעפצקרשת'\n",
        "    result = 0\n",
        "    try:\n",
        "        for l in letters:\n",
        "            n = alphabet.index(l) + 1\n",
        "            d = int(n/10)\n",
        "            result += (int(n%10) + d) * (10 ** d)\n",
        "    except:\n",
        "        print('Wrong parameter, hebrew letters only expected! Got: ', letters)\n",
        "    return result\n",
        "\n",
        "accent_names = np.asarray([\n",
        "    '[START]',\n",
        "    '[END]',\n",
        "    \"HEBREW ACCENT ETNAHTA\",\n",
        "    \"HEBREW ACCENT SEGOL\",\n",
        "    \"HEBREW ACCENT SHALSHELET\",\n",
        "    \"HEBREW ACCENT ZAQEF QATAN\",\n",
        "    \"HEBREW ACCENT ZAQEF GADOL\",\n",
        "    \"HEBREW ACCENT TIPEHA\",\n",
        "    \"HEBREW ACCENT REVIA\",\n",
        "    \"HEBREW ACCENT ZARQA\",\n",
        "    \"HEBREW ACCENT PASHTA\",\n",
        "    \"HEBREW ACCENT YETIV\",\n",
        "    \"HEBREW ACCENT TEVIR\",\n",
        "    \"HEBREW ACCENT GERESH\",\n",
        "    \"HEBREW ACCENT GERESH MUQDAM\",\n",
        "    \"HEBREW ACCENT GERSHAYIM\",\n",
        "    \"HEBREW ACCENT QARNEY PARA\",\n",
        "    \"HEBREW ACCENT TELISHA GEDOLA\",\n",
        "    \"HEBREW ACCENT PAZER\",\n",
        "    \"HEBREW ACCENT ATNAH HAFUKH\",\n",
        "    \"HEBREW ACCENT MUNAH\",\n",
        "    \"HEBREW ACCENT MAHAPAKH\",\n",
        "    \"HEBREW ACCENT MERKHA\",\n",
        "    \"HEBREW ACCENT MERKHA KEFULA\",\n",
        "    \"HEBREW ACCENT DARGA\",\n",
        "    \"HEBREW ACCENT QADMA\",\n",
        "    \"HEBREW ACCENT TELISHA QETANA\",\n",
        "    \"HEBREW ACCENT YERAH BEN YOMO\",\n",
        "    \"HEBREW ACCENT OLE\",\n",
        "    \"HEBREW ACCENT ILUY\",\n",
        "    \"HEBREW ACCENT DEHI\",\n",
        "    \"HEBREW ACCENT ZINOR\",\n",
        "    \"HEBREW POINT METEG\",\n",
        "    \"HEBREW PUNCTUATION PASEQ\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actual function that generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data():\n",
        "    books = [\n",
        "             'בראשית',\n",
        "             'שמות',\n",
        "             'ויקרא',\n",
        "             'במדבר',\n",
        "             'דברים',\n",
        "    ]\n",
        "    aliyot = {\n",
        "        'ראשון' : 1,\n",
        "        'שני': 2,\n",
        "        'שלישי': 3,\n",
        "        'רביעי': 4,\n",
        "        'חמישי': 5,\n",
        "        'שישי': 6,\n",
        "        'שביעי': 7,\n",
        "    }\n",
        "    EXPECTED_SAMPLE_RATE = 16000\n",
        "\n",
        "    for book_name in books:\n",
        "        chapters_in_book = []\n",
        "        with open(os.path.join('outputs', book_name + '.json'), 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "        pasuk_counter = 0\n",
        "        for chap in data['chapters']:\n",
        "            psukim = data['psukim'][pasuk_counter:pasuk_counter + data['chapters'][chap]]\n",
        "            chapters_in_book.append(psukim)\n",
        "            pasuk_counter += data['chapters'][chap]\n",
        "                  \n",
        "        for w_chap in data['weekly_chaps']:\n",
        "            if w_chap in data['double_chaps']: continue\n",
        "            for alia in data['weekly_chaps'][w_chap]['aliyot']:\n",
        "                alia_p = []\n",
        "                f = data['weekly_chaps'][w_chap]['aliyot'][alia][0].split('-')\n",
        "                to = data['weekly_chaps'][w_chap]['aliyot'][alia][1].split('-')\n",
        "                f_chap = gimatria(f[0])\n",
        "                f_pasuk = gimatria(f[1][1:-1])\n",
        "                to_chap = gimatria(to[0])\n",
        "                to_pasuk = gimatria(to[1][1:-1])\n",
        "                \n",
        "                if f_chap == to_chap:\n",
        "                    alia_p = chapters_in_book[f_chap - 1][f_pasuk - 1:to_pasuk]\n",
        "                else:\n",
        "                    for c in range(f_chap - 1, to_chap):\n",
        "                        if c == f_chap - 1:\n",
        "                            for i in range(f_pasuk - 1, len(chapters_in_book[c])):\n",
        "                                alia_p.append(chapters_in_book[c][i])\n",
        "                        elif c == to_chap - 1:\n",
        "                            for i in range(to_pasuk):\n",
        "                                alia_p.append(chapters_in_book[c][i])\n",
        "                        else:\n",
        "                            for i in chapters_in_book[c]:\n",
        "                                alia_p.append(i)\n",
        "                alia_accents = []\n",
        "                for pasuk in alia_p:\n",
        "                    for char in pasuk:\n",
        "                        if unicodedata.name(char) in accent_names:\n",
        "                            accent_vector = (accent_names == unicodedata.name(char)).astype(int)\n",
        "                            alia_accents.append(accent_vector)\n",
        "                #Add [START] and [END] vectors\n",
        "                start = (accent_names == '[START]').astype(int)\n",
        "                end = (accent_names == '[END]').astype(int)\n",
        "                alia_accents = [start] + alia_accents + [end]\n",
        "\n",
        "                audio_file = os.path.join('inputs', book_name, w_chap[5:], str(aliyot[alia]) + '.mp3')\n",
        "                y, sr = librosa.load(audio_file, EXPECTED_SAMPLE_RATE)\n",
        "                if len(y.shape) == 2:\n",
        "                    y = y.mean(1)\n",
        "                y = y.astype(np.float32)\n",
        "                all_audio_data_after_augumentation = [\n",
        "                                                      y,\n",
        "                                                      add_noise(y, 0.05),\n",
        "                                                      change_pitch(y, sr, 4),\n",
        "                                                      change_pitch(y, sr, -6),\n",
        "                                                      change_speed(y, 1.5),\n",
        "                                                      change_speed(y, .75),\n",
        "                ]\n",
        "                for aud_data in all_audio_data_after_augumentation:\n",
        "                    if aud_data.shape[0] % EXPECTED_SAMPLE_RATE:\n",
        "                        zero_padding = EXPECTED_SAMPLE_RATE - (aud_data.shape[0] % EXPECTED_SAMPLE_RATE)\n",
        "                        aud_data = np.pad(aud_data, (0, zero_padding))\n",
        "                    aud_data = np.reshape(aud_data, (aud_data.shape[0] // EXPECTED_SAMPLE_RATE, EXPECTED_SAMPLE_RATE))\n",
        "                    seq_of_specs = []\n",
        "                    for frame in aud_data:\n",
        "                        spectrogram = tf.signal.stft(frame, frame_length=255, frame_step=128)\n",
        "                        spectrogram = tf.abs(spectrogram)\n",
        "                        spectrogram = tf.reshape(spectrogram, spectrogram.shape + (1))\n",
        "                        seq_of_specs.append(np.asarray(spectrogram))\n",
        "                    yield seq_of_specs, alia_accents\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Here we build our generator for `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = generate_data() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53qRE0n-hRV_"
      },
      "source": [
        "## And, finaly we will built our training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwxDdPcy8Exd",
        "outputId": "81b8c16f-c4c0-46d6-9544-b14489e7ffce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, None, 124,   0           []                               \n",
            "                                129, 1)]                                                          \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, None, 1, 1,   977920      ['input[0][0]']                  \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, 1, 1,   0           ['sequential[0][0]']             \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " time_distributed_5 (TimeDistri  (None, None, 256)   0           ['dropout[0][0]']                \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " decoder_input (InputLayer)     [(None, None, 34)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 34),         39576       ['time_distributed_5[0][0]']     \n",
            "                                 (None, 34),                                                      \n",
            "                                 (None, 34)]                                                      \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, None, 34)     9384        ['decoder_input[0][0]',          \n",
            "                                                                  'lstm[0][1]',                   \n",
            "                                                                  'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 34)     1190        ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,028,070\n",
            "Trainable params: 1,028,070\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_labels = len(accent_names)\n",
        "enc_input = Input(shape=(None, 124, 129, 1), name='input', dtype='float32')\n",
        "\n",
        "enc_conv = Sequential([Conv2D(32, 3),\n",
        "ReLU(0.01),\n",
        "TimeDistributed(MaxPool2D(2, padding='same')),\n",
        "Conv2D(64, 3),\n",
        "ReLU(0.01),\n",
        "TimeDistributed(MaxPool2D(2, padding='same')),\n",
        "Conv2D(128, 3),\n",
        "ReLU(0.01),\n",
        "TimeDistributed(MaxPool2D(2, padding='same')),\n",
        "Conv2D(256, 3),\n",
        "ReLU(0.01),\n",
        "TimeDistributed(MaxPool2D(3, padding='same')),\n",
        "Conv2D(256, 3),\n",
        "ReLU(0.01),\n",
        "TimeDistributed(tf.keras.layers.MaxPool2D(3, padding='same'))])(enc_input)\n",
        "#Output shape is (batch, sequence_length, 1, 1, 256)\n",
        "\n",
        "enc_drop = Dropout(0.2)(enc_conv)\n",
        "enc_reshape = TimeDistributed(Reshape((256,)))(enc_drop)\n",
        "\n",
        "enc_output, enc_h, enc_c = LSTM(num_labels, return_state=True)(enc_reshape)\n",
        "\n",
        "dec_input = Input(\n",
        "    shape=(None, num_labels),\n",
        "    name='decoder_input',\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "decoder = LSTM(num_labels, return_sequences=True)(dec_input, initial_state=[enc_h, enc_c])\n",
        "decoder = Dense(num_labels)(decoder)\n",
        "\n",
        "model = Model([enc_input, dec_input], decoder)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now we are training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AkwFz1vv0QL",
        "outputId": "53af2683-97e2-48ca-d33e-8ffade16f6b7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "adam = tf.keras.optimizers.Adam()\n",
        "accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "model.compile(adam, 'categorical_crossentropy')\n",
        "\n",
        "i = 0\n",
        "for inp, tar in dataset:\n",
        "    start = time.time()\n",
        "    inp = np.asarray([inp])\n",
        "    dec_inp = np.asarray([tar[:-1]])\n",
        "    tar = np.asarray([tar[1:]])\n",
        "    accuracy.reset_state()\n",
        "    loss = model.train_on_batch([inp, dec_inp], tar)\n",
        "    output = model.predict([inp, dec_inp])\n",
        "    accuracy.update_state(tar, output)\n",
        "    print(f'Step {i+1}, time taken {time.time()-start:.2f} sec, Loss {loss:.2f}, Accuracy {accuracy.result():.2f}\\r')\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7muNEoGE3op"
      },
      "source": [
        "## For now we just save the weights for future work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcbDlp2qayFE"
      },
      "outputs": [],
      "source": [
        "model.save_weights('askenaz_taamim_model.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_hebrew_accents_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d2859beb45dd9968c1d74c07ed5bb99ff725817d6d4c47a5aea139339d5b4f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
